{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIT_BrainMusic_Train_polyphony_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brainmusic/models/blob/master/BrainMusic_Train_polyphony_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4ZJ3q5hftP",
        "colab_type": "text"
      },
      "source": [
        "Licensed under the Apache [License](https://www.apache.org/licenses)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8lGTfVviZ_R",
        "colab_type": "text"
      },
      "source": [
        "# Brain Music. Polyphony RNN Training\n",
        "Raquel Bujalance and [Cecil Fernandez Briche ](https://github.com/gabaxi)\n",
        "\n",
        "This colab notebook lets you to train Polyphony RNN for music generation, based on [Magenta library](https://github.com/tensorflow/magenta/tree/master/magenta/models/polyphony_rnn).This model applies language modeling using an LSTM to polyphonic music generation.\n",
        "\n",
        "The notebook has been created to help anyone who wants to try training their own polyphony model using Magenta library, step by step, in colab.\n",
        "\n",
        "We have trained it with different samples linked to emotions, you can see the results in [post].\n",
        "\n",
        "\n",
        "Instructions for running:\n",
        "Make sure to use a GPU runtime, click: Runtime >> Change Runtime Type >> GPU\n",
        "\n",
        "Double-click any of the hidden cells to view the code\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQgJ6x2UtPCs",
        "colab_type": "text"
      },
      "source": [
        "# Environment Setup\n",
        "Install magenta and fluidsynth, a sequence synthesis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uq7cNslgQQS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install\n",
        "\n",
        "#@markdown Install magenta and fluidsynth as a synthesizer to listen de audios.\n",
        " #@markdown Magenta is compatible with both Python 2 and 3.\n",
        " #@markdown This take some time, specially for fluidsynth instalation\n",
        "\n",
        "!pip install magenta\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 \\\n",
        "fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi\n",
        "# Hack to allow python to pick up the newly-installed fluidsynth lib. \n",
        "# This is only needed for the hosted Colab environment.\n",
        "import ctypes.util\n",
        "orig_ctypes_util_find_library = ctypes.util.find_library\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return orig_ctypes_util_find_library(lib)\n",
        "ctypes.util.find_library = proxy_find_library"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx1ptrjdi3A4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Drive Setup\n",
        "#@markdown If your training sample is in google drive you need to connect it. \n",
        "#@markdown You can also upload the data to a temporary folder but it will be \n",
        "#@markdown lost when the session is closed.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O976jh2chLLp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Import Dependencies\n",
        "#@markdown Import libraries from Magenta, Tensorflow and Numpy\n",
        "\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import magenta.music as mm\n",
        "import magenta\n",
        "from magenta.scripts import convert_dir_to_note_sequences\n",
        "from magenta.models.polyphony_rnn import *\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-JSJ5d8xkXJ",
        "colab_type": "text"
      },
      "source": [
        "# Sample adaptation\n",
        "\n",
        "Magenta does not work directly with Midi files but with NoteSequences, so the first step to create the sample is to convert the Midi files into Note sequences files and pack them as tfrecord to work with them fast and efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqVTh_Xs56vN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "You need to define the folder with the midi files, as well as the\n",
        "folder in which the notesequence will be saved. Change the routes from below to your own folders. The output of this step (notesequences)  can be saved directly as a temporary file, e.g. /tmp/notesequences_Happy.tfrecord \\ instead of on the drive, since after the next step in which the sample is split it would no longer be necessary.  \n",
        "\n",
        "Tip for novices: You can see the path of the folder to the left in files and with the mouse button select \"copy path\". As the route usually contains   \"My drive\" remember to enter \"\\\\\"  in the middel as \"My \\ drive\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksd7ZJLG55Ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!convert_dir_to_note_sequences \\\n",
        "--input_dir=/content/drive/My\\ Drive/brain_music/data/audios/Happy \\\n",
        "--output_file=/content/drive/My\\ Drive/brain_music/data/audios/noteSequences/notesequences_Happy.tfrecord \\\n",
        "--log=INFO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMfaieWF6tLe",
        "colab_type": "text"
      },
      "source": [
        "Now you are ready to separate your sample between train and test. The percentage you leave in the test for evaluate your model is defined with the eval ratio argument. For example with a eval ratio equal to 10%, the 90% of the sample will be saved in the traing collection, while the remaining 10% will be stored as evaluation sample.\n",
        "\n",
        "The input for this step must match the one defined as output in the previous step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIC9rvO2FfiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test and train sample split wih 10% ratio\n",
        "!polyphony_rnn_create_dataset \\\n",
        "--input=/content/drive/My\\ Drive/brain_music/data/audios/noteSequences/notesequences_Happy.tfrecord \\\n",
        "--output_dir=/content/drive/My\\ Drive/brain_music/RNN/sample/Happy \\\n",
        "--eval_ratio=0.10 \\\n",
        "--config='polyphony'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5VWN5k87qM",
        "colab_type": "text"
      },
      "source": [
        "If the cell has been executed correctly, you have to have two files saved in the output_dir, both in tfrecord format, one with the training sample and one with the eval sample. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0-HHl_H5bKE",
        "colab_type": "text"
      },
      "source": [
        "# Model Training\n",
        "Now you are ready to train your model!\n",
        "\n",
        "This step can take a long, long time and depending on how large your database is and the number of layers and their size, you may get a memory error, or lose the connection to the GPU.\n",
        "\n",
        "We recommend you to start with a small sample and a light model for example a bach size of 64 and two LSTM rnn layers of 64, \"batch_size=64,rnn_layer_sizes=[64,64]\" and incorporate more complexity little by little. If you save the checkpoints you can re-launch the training at the point where you left the previous session, this is especially interesting if you lose the web connection or your session closes unexpectedly.  \n",
        "\n",
        "\n",
        "To train the model you can define the following parameters: \n",
        "* **run_dir** is the directory where checkpoints and TensorBoard data will be stored.\n",
        "* **sequence_example_file** is the TFRecord file with the train sample, the folder  must be the same as the one defined in output_dir in the previous step . \n",
        "* **num_training_steps** is an optional parameter for how many update steps to take before exiting the training loop. By default, training will run continuously until manually terminated.\n",
        "* **hparams** is another optional parameter that specifies the hyperparameters you want to use. By default polyphony rnn model use this configuration \n",
        "* **dropout_keep_prob** is a optional parameter to reduce overfitting and improving model performance. Dropout is a regularization method to select randomly a % of neurons in the LSTM units thats are probabilistically excluded from activation and weight updates while training the model. \n",
        "* **learning_rate** is another optional parameter that controls how quickly or slowly a neural network model learns. This value are ussually between 0.0 and 1.0, a learning rate too small may result in a long training process that could get stuck, whereas a value too large may result in an unstable training process.\n",
        "\n",
        "By default polyphony_rnn model use this configuration: \n",
        "* batch_size=64,\n",
        "* rnn_layer_sizes=[256, 256, 256]\n",
        "* dropout_keep_prob=0.5\n",
        "* learning_rate=0.001\n",
        "\n",
        "Tip for novices: if you change the hyperparameters, for example by increasing the number of layers, remember to change the directory where the checkpoints are stored, otherwise the model will try to link to the last training and will give you an error of layer dimensions. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9LAnyFhPUOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train the model!\n",
        "!polyphony_rnn_train \\\n",
        "--run_dir=/content/drive/My\\ Drive/brain_music/RNN/modelos/polyphony/happy/run2 \\\n",
        "--sequence_example_file=/content/drive/My\\ Drive/brain_music/RNN/sample/Happy/training_poly_tracks.tfrecord \\\n",
        "--num_training_steps=10000 \\\n",
        "--hparams=\"batch_size=64,rnn_layer_sizes=[128,128,128]\" \\\n",
        "--config='polyphony' \\\n",
        "--num_checkpoints=10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMi2Cd4MleX",
        "colab_type": "text"
      },
      "source": [
        "When you consider that the model is sufficiently tuned you can keep it in a bundle file. This allows you to import the trained model at any time and use it to create new sequences. To save it you have to call the same function of the previous step polyphony_rnn_generate, but changing some of the parameterization\n",
        "\n",
        "*   the run directory has to be the same as in previous step \n",
        "*   hparam must also be the same as those defined in the training. \n",
        "*   bundle_file is the path where to save the file with the model.mag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilUtMeudzrHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save your model \n",
        "!melody_rnn_generate \\\n",
        "--run_dir=/content/drive/My\\ Drive/brain_music/RNN/modelos/polyphony/happy/run2 \\\n",
        "--hparams=\"batch_size=64,rnn_layer_sizes=[128,128,128]\" \\\n",
        "--bundle_file=/content/braimusic_poly_happy_rnn.mag \\\n",
        "--save_generator_bundle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6O27QcYPMQS",
        "colab_type": "text"
      },
      "source": [
        "# Generating polyphonic tracks \n",
        "New tracks can be generated from the last saved checkpoint of the model (save in run_dir) or from the bundle, here is an example of both features. \n",
        "In addition the sequence can be started from the first notes of a midi file or directly by giving the notes. \n",
        "\n",
        "##Generation from a check point\n",
        "When you create a new melody from the last checkpoint trained you can do it at the end of the process or during the training to analyze the fit of the model. The training function also allows you to evaluate the model in the test sample, but what better test than the human ear? \n",
        "In fact this type of models are usually evaluated through listening test when  participants are asked to rate the generated sample in terms of the Likert scale, see for example the evaluation of [music transformed model](https://arxiv.org/pdf/1809.04281.pdf). \n",
        "As in the previous casesthe run_dir must be the same path where the checkpoints have been saved, in output_dir you must put the path of the directory where you want to save the new creations. \n",
        "num_outputs gives the number of samples to generate and num_steps the length of the track. \n",
        "In this case, the generation is produced from three notes in Midi [language](https://newt.phys.unsw.edu.au/jw/notes.html) inserted as primer_pitches.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkcTsnITxCWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generar nuevas canciones con el modelo entrenado a partir de secuencia de notas\n",
        "!polyphony_rnn_generate \\\n",
        "--config='polyphony_rnn ' \\\n",
        "--run_dir=/content/drive/My\\ Drive/brain_music/RNN/modelos/polyphony/happy/run2 \\\n",
        "--output_dir=/content/drive/My\\ Drive/brain_music/data/audios/creacionesAI/poly_train \\\n",
        "--num_outputs=3 \\\n",
        "--num_steps=600 \\\n",
        "--primer_pitches=\"[67,64,60]\" \\\n",
        "--condition_on_primer=true \\\n",
        "--inject_primer_during_generation=false"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2CSswPPxTNH",
        "colab_type": "text"
      },
      "source": [
        "But to create a richer structure, it is better to start the sequence with a few seconds of a real track, this can be done by replacing primer_pitches by primer_midi, you have included the midi path you want to use. \n",
        "It is recommended to use only a few seconds, so before using it you can use the magenta library to extract some notes. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIV4Ipj45poj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate new sequences with the train model (budle_file)\n",
        "!polyphony_rnn_generate \\\n",
        "--config='polyphony_rnn ' \\\n",
        "--run_dir=/content/drive/My\\ Drive/brain_music/RNN/modelos/polyphony/happy/run2 \\\n",
        "--output_dir=/content/drive/My\\ Drive/brain_music/data/audios/creacionesAI/polyphony_train \\\n",
        "--num_outputs=3 \\\n",
        "--num_steps=600 \\\n",
        "--primer_midi=/content/drive/My\\ Drive/brain_music/happy_1.mid \\\n",
        "--condition_on_primer=true \\\n",
        "--inject_primer_during_generation=false"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK73ejHc2dKB",
        "colab_type": "text"
      },
      "source": [
        "Ready to listen your new songs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQZjFHVx0tQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_sequences = (\"/content/drive/My Drive/brain_music/data/audios/creacionesAI/polyphony_train\")\n",
        "for ns in generated_sequences:\n",
        "  # print(ns)\n",
        "  mm.plot_sequence(ns)\n",
        "  mm.play_sequence(ns, synth=mm.fluidsynth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNOlRkY23UZf",
        "colab_type": "text"
      },
      "source": [
        "#Generation from a Bundle\n",
        "Creating songs from a saved bundle is just as easy as changing run_dir for bundle_file, with the folder path where the bundle is stored.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxy04r4U3c3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generate new sequences with the train model (budle_file)\n",
        "!polyphony_rnn_generate \\\n",
        "--config='polyphony_rnn ' \\\n",
        "--bundle_file=/content/braimusic_poly_happy_rnn.mag  \\\n",
        "--output_dir=/content/drive/My\\ Drive/brain_music/data/audios/creacionesAI/polyphony_train \\\n",
        "--num_outputs=3 \\\n",
        "--num_steps=600 \\\n",
        "--primer_midi=/content/drive/My\\ Drive/brain_music/happy_1.mid \\\n",
        "--condition_on_primer=true \\\n",
        "--inject_primer_during_generation=false"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTtMB8Zsv4VD",
        "colab_type": "text"
      },
      "source": [
        "#Model Comparison\n",
        "\n",
        "Compare your creations with those of the model pretrained by Magenta, to do it you only need to load the magenta bundle. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmtEnL2u5ERV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#modelo pre-entrenado\n",
        "from magenta.models.polyphony_rnn import polyphony_sequence_generator\n",
        "mm.notebook_utils.download_bundle('polyphony_rnn.mag', '/content/')\n",
        "bundle_polyphony = mm.sequence_generator_bundle.read_bundle_file('/content/polyphony_rnn.mag')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6FVbWFbOmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!polyphony_rnn_generate \\\n",
        "--config='polyphony_rnn ' \\\n",
        "--bundle_file=/content/polyphony_rnn.mag  \\\n",
        "--output_dir=/content/drive/My\\ Drive/brain_music/data/audios/creacionesAI/polyphony_pretrain \\\n",
        "--num_outputs=3 \\\n",
        "--num_steps=600 \\\n",
        "--primer_midi=/content/drive/My\\ Drive/brain_music/happy_1.mid \\\n",
        "--condition_on_primer=true \\\n",
        "--inject_primer_during_generation=false"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh2rws2_0wcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_sequences = (\"/content/drive/My\\ Drive/brain_music/data/audios/creacionesAI/polyphony_pretrain\")\n",
        "for ns in generated_sequences:\n",
        "  # print(ns)\n",
        "  mm.plot_sequence(ns)\n",
        "  mm.play_sequence(ns, synth=mm.fluidsynth)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}